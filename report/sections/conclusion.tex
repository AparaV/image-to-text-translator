%=====================
%  CONCLUSION
%=====================
Our results show that it is possible to classify images using the idea of Support Vector Machines i.e., by transforming the pixel values and constructing the optimal separating hyperplane. We saw that the Kernel matrix associated with these transformation can be constructed by any positive semidefinite matrix i.e., a valid transformation is obtained by relaxing the positive definiteness property associated with an inner product. By making a linear transformation (using raw pixel values), our support vector machine has an accuracy of $86.49$ \%. With polynomial transformations of degree $2$ and $3$, we got an accuracy of $98.08$ \% and $97.85$ \% respectively. As expected, making a \textit{wrong} transformation can make it impossible to construct a hyperplane. Our results with the sigmoid transformation support our belief with a reduced accuracy of just $10.09$ \%. A key idea we learned from this project was that it is possible to reduce a nonlinear problem into a linear version that can be solved using existing, or simpler techniques. 

We experimented with $4$ different transformations, each with constructing $10$ hyperplanes. We can improve upon this design by using different kernels for each of the $10$ different hyperplanes i.e., one number that we are classifying, say $3$, could be in a different space than another number, say $5$. So our model would perform better by constructing a hyperplane in the linear transformation for classifying $3$, and a hyperplane in the polynomial transformation for classifying $5$.

This technique is very much relevant today and has many applications. Moving forward, we can also build classifiers for the alphabets. Then, we can use the sliding window technique (scanning each $m\times m$ pixel box in an image) on an image with text to classify and read the text present in the image. Support Vector Machines can also be used in other classification problems such as predicting if a patient has a tumor, or not; music recommendation and; cataloging images of different animal species. The applications are countless.

Currently, the multi-class classification problem requires constructing $k$ different hyperplanes. Future research in this area could be studying if it is possible to construct a single set of $m < k$ hyperplanes that form a box that encloses and separates the different classes, and if that method will be a more efficient way for approaching the multi-class classification problem than the one-versus-rest technique.